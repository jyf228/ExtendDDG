import torch
from bert_score import score
from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu
from rouge_score import rouge_scorer


def detailed_evaluate_description(
    generated_description: str, ground_truth: str
) -> dict[str, float]:
    bert_score = _compute_bert(generated_description, ground_truth)
    rouge_score = _compute_rouge(generated_description, ground_truth)
    bleu_score = _compute_bleu(generated_description, ground_truth)

    return bert_score | rouge_score | {'BLEU': bleu_score}


def _compute_bert(
    generated_description: str, ground_truth: str, model = "bert-base-uncased"
) -> dict[str, float]:
    """
    Evaluate the generated description using BERTScore.

    Args:
        generated_description (str): The description generated by the model.
        ground_truth (str): The ground truth description for comparison.

    Returns:
        float: The BERTScore F1 score between the generated and ground truth descriptions.
    """

    P, R, F1 = score(
        [generated_description], [ground_truth],
        lang="en",
        model_type=model,
        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    )

    return {'precision': float(P.item()), 'recall': float(R.item()), 'f1': float(F1.item())}


def _compute_rouge(
    generated_description: str, ground_truth: str
) -> dict[str, float]:
    """
    Evaluate the generated description using ROUGE.

    Args:
        generated_description (str): The description generated by the model.
        ground_truth (str): The ground truth description for comparison.

    Returns:
        dict: A dictionary containing ROUGE scores.
    """

    scorer = rouge_scorer.RougeScorer(
        ['rouge1', 'rouge2', 'rougeL'],
        use_stemmer=True
    )

    scores = scorer.score(ground_truth, generated_description)

    result = {}
    result["ROUGE-1:"] = scores['rouge1'].fmeasure
    result["ROUGE-2:"] = scores['rouge2'].fmeasure
    result["ROUGE-L:"] = scores['rougeL'].fmeasure

    return result


def _compute_bleu(generated_description: str, ground_truth: str) -> float:
    """
    Evaluate the generated description using BLEU score.

    Args:
        generated_description (str): The description generated by the model.
        ground_truth (str): The ground truth description for comparison.

    Returns:
        float: The BLEU score between the generated and ground truth descriptions.
    """

    # BLEU needs tokenized sentences
    reference_tokens = ground_truth.split()
    candidate_tokens = generated_description.split()

    # Smoothing helps avoid BLEU = 0 when sentences are short
    smooth = SmoothingFunction().method1

    score = sentence_bleu(
        [reference_tokens],
        candidate_tokens,
        smoothing_function=smooth
    )

    return score
