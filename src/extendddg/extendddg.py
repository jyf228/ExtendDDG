from typing import Any, Tuple
import torch
from autoddg import AutoDDG, GPTEvaluator
from pandas import DataFrame
from bert_score import score
from rouge_score import rouge_scorer
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

class ExtendDDG:
    """ExtendDDG: Extended Version of AutoDDG for Supplemental Dataset Documentation

    Args:
        client (Any): OpenAI-compatible client (e.g. ``openai.OpenAI(...)``).
        model_name (str): Default model identifier (e.g. ``"gpt-4o"``).
        TODO: Add additional parameters.

    Examples:
        TODO: Add example usage.
    """

    def __init__(
        self,
        client: Any,
        model_name: str,
    ):
        self.client = client
        self.model_name = model_name
        self.auto_ddg = AutoDDG(client=client, model_name=model_name)

    def describe_dataset(
        self,
        dataset_sample: str,
        dataset_profile: str | None = None,
        use_profile: bool = False,
        semantic_profile: str | None = None,
        use_semantic_profile: bool = False,
        data_topic: str | None = None,
        use_topic: bool = False,
    ) -> Tuple[str, str]:
        # NOTE: Probably doesn't need to be modified if the profilers and topic generator already incorporate the supplemental data.
        # But should experiment with including it directly in the prompt if description quality doesn't improve much with the initial changes.
        return self.auto_ddg.describe_dataset(
            dataset_sample=dataset_sample,
            dataset_profile=dataset_profile,
            use_profile=use_profile,
            semantic_profile=semantic_profile,
            use_semantic_profile=use_semantic_profile,
            data_topic=data_topic,
            use_topic=use_topic,
        )

    def profile_dataframe(self, dataframe: DataFrame) -> Tuple[str, str]:
        # TODO: This will probably not need to change, but can experiment with incorporating the supplemental data.
        return self.auto_ddg.profile_dataframe(dataframe)

    def analyze_semantics(self, dataframe: DataFrame) -> str:
        # TODO: Incorporate supplemental data into the semantic profiler.
        return self.auto_ddg.analyze_semantics(dataframe)

    def generate_topic(
        self, title: str, original_description: str | None, dataset_sample: str
    ) -> str:
        # TODO: Experiment with whether this changes/improves with supplemental data.
        return self.auto_ddg.generate_topic(
            title=title,
            original_description=original_description,
            dataset_sample=dataset_sample,
        )

    def expand_description_for_search(self, description: str, topic: str) -> Tuple[str, str]:
        return self.auto_ddg.expand_description_for_search(description, topic)

    def evaluate_description(self, description: str) -> str:
        return self.auto_ddg.evaluate_description(description)

    def set_evaluator(self, evaluator: GPTEvaluator) -> None:
        self.auto_ddg.set_evaluator(evaluator)
        
    def detailed_evaluate_description(self, generated_description: str, ground_truth: str) -> dict[str, float]:
        bert_score = self.bert_evaluate_description(generated_description, ground_truth)
        rouge_score = self.rouge_evaluate_description(generated_description, ground_truth)
        bleu_score = self.bleu_evaluate_description(generated_description, ground_truth)
        
        return bert_score | rouge_score | {'BLEU': bleu_score}
    
    def bert_evaluate_description(self, generated_description: str, ground_truth: str, model="bert-base-uncased") -> dict[str, float]:
        """Evaluate the generated description using BERTScore.

        Args:
            generated_description (str): The description generated by the model.
            ground_truth (str): The ground truth description for comparison.

        Returns:
            float: The BERTScore F1 score between the generated and ground truth descriptions.
        """
        # Your texts
        candidate = "your generated answer text here"
        reference = "your ground-truth reference text here"

        # Compute BERTScore
        P, R, F1 = score([generated_description], [ground_truth], 
                         lang="en", 
                         model_type=model,
                         device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))

        # print("Precision:", P.item())
        # print("Recall:", R.item())
        # print("F1:", F1.item())
        return {'precision': float(P.item()), 'recall': float(R.item()), 'f1': float(F1.item())}
    
    def rouge_evaluate_description(self, generated_description: str, ground_truth: str) -> dict[str, float]:
        """Evaluate the generated description using ROUGE.

        Args:
            generated_description (str): The description generated by the model.
            ground_truth (str): The ground truth description for comparison.

        Returns:
            dict: A dictionary containing ROUGE scores.
        """

        scorer = rouge_scorer.RougeScorer(
            ['rouge1', 'rouge2', 'rougeL'], 
            use_stemmer=True
        )

        scores = scorer.score(ground_truth, generated_description)

        result = dict()
        result["ROUGE-1:"] = scores['rouge1'].fmeasure
        result["ROUGE-2:"] = scores['rouge2'].fmeasure
        result["ROUGE-L:"] = scores['rougeL'].fmeasure

        return result
    
    def bleu_evaluate_description(self, generated_description: str, ground_truth: str) -> float:
        """Evaluate the generated description using BLEU score.

        Args:
            generated_description (str): The description generated by the model.
            ground_truth (str): The ground truth description for comparison.

        Returns:
            float: The BLEU score between the generated and ground truth descriptions.
        """
        # BLEU needs tokenized sentences
        reference_tokens = ground_truth.split()
        candidate_tokens = generated_description.split()

        # Smoothing helps avoid BLEU = 0 when sentences are short
        smooth = SmoothingFunction().method1

        score = sentence_bleu(
            [reference_tokens], 
            candidate_tokens,
            smoothing_function=smooth
        )

        return score